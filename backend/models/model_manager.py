"""
Model Manager - Orchestrates all fraud detection models

Manages:
- Basic Rules Engine (v1.0.0)
- Machine Learning Model (v2.1.0) 
- Deep Learning Model (v3.0.0)

Handles model selection, performance tracking, and comparison
"""
from typing import Dict, List, Tuple
import numpy as np
from datetime import datetime, timedelta
import random

from .rules_engine import RulesEngineModel
from .ml_model import MLModel
from .dl_model import DLModel


class ModelManager:
    """Manages all fraud detection models"""
    
    def __init__(self):
        # Initialize all three models
        print("ðŸš€ Initializing Fraud Detection Models...")
        
        self.rules_engine = RulesEngineModel()
        self.ml_model = MLModel()
        self.dl_model = DLModel()
        
        # Model registry
        self.models = {
            "rules_engine": self.rules_engine,
            "ml_model": self.ml_model,
            "dl_model": self.dl_model,
        }
        
        # Active model (for blocking decisions)
        self.active_model = "ml_model"  # Default to ML model
        
        # Historical performance data for charts
        self._initialize_historical_data()
        
        print("âœ… All models initialized successfully")
    
    def predict_all(self, features: np.ndarray) -> Dict[str, Dict]:
        """
        Run prediction through all models
        
        Args:
            features: Transaction features [Time, V1-V28, Amount]
        
        Returns:
            Dict with predictions from all models
        """
        results = {}
        
        for model_name, model in self.models.items():
            try:
                proba = model.predict_proba(features)
                fraud_prob = float(proba[0][1])  # Probability of fraud
                is_fraud = fraud_prob > 0.5
                
                results[model_name] = {
                    "fraud_probability": fraud_prob,
                    "is_fraud": is_fraud,
                    "risk_score": fraud_prob * 100,  # Convert to percentage
                }
            except Exception as e:
                print(f"âŒ Error in {model_name}: {e}")
                results[model_name] = {
                    "fraud_probability": 0.0,
                    "is_fraud": False,
                    "risk_score": 0.0,
                    "error": str(e)
                }
        
        # Add active model decision
        results["active_model"] = self.active_model
        results["blocking_decision"] = results[self.active_model]["is_fraud"]
        
        return results
    
    def set_active_model(self, model_name: str) -> bool:
        """
        Change the active model
        
        Args:
            model_name: One of 'rules_engine', 'ml_model', 'dl_model'
        
        Returns:
            True if successful, False otherwise
        """
        if model_name in self.models:
            self.active_model = model_name
            print(f"âœ… Active model changed to: {model_name}")
            return True
        else:
            print(f"âŒ Invalid model name: {model_name}")
            return False
    
    def get_all_metrics(self) -> Dict:
        """Get performance metrics for all models"""
        metrics = {}
        
        for model_name, model in self.models.items():
            model_metrics = model.get_metrics()
            
            # Add active status
            model_metrics["is_active"] = (model_name == self.active_model)
            
            metrics[model_name] = model_metrics
        
        return metrics
    
    def get_comparison_data(self) -> Dict:
        """
        Get model comparison data for the dashboard
        
        Returns comprehensive comparison including:
        - Current metrics
        - Historical trends
        - Performance charts data
        """
        metrics = self.get_all_metrics()
        
        comparison = {
            "models": [
                {
                    "id": "rules_engine",
                    "name": "Basic Rules Engine",
                    "version": "1.0.0",
                    "is_active": self.active_model == "rules_engine",
                    "metrics": metrics["rules_engine"],
                },
                {
                    "id": "ml_model",
                    "name": "Machine Learning Model",
                    "version": "2.1.0",
                    "is_active": self.active_model == "ml_model",
                    "metrics": metrics["ml_model"],
                },
                {
                    "id": "dl_model",
                    "name": "Deep Learning Model",
                    "version": "3.0.0",
                    "is_active": self.active_model == "dl_model",
                    "metrics": metrics["dl_model"],
                }
            ],
            "historical_data": self.historical_data,
            "best_model": self._get_best_model(),
        }
        
        return comparison
    
    def _initialize_historical_data(self):
        """
        Initialize historical performance data for charts
        In production, this would be loaded from database
        """
        weeks = ["Week 1", "Week 2", "Week 3", "Week 4"]
        
        # Accuracy trends (improving over time)
        self.historical_data = {
            "accuracy_trends": {
                "labels": weeks,
                "datasets": [
                    {
                        "label": "Model v1.0 (Rules)",
                        "data": [82, 84, 85, 85],
                        "color": "#ef4444",  # Red
                    },
                    {
                        "label": "Model v2.1 (ML)",
                        "data": [88, 90, 91, 92],
                        "color": "#3b82f6",  # Blue
                    },
                    {
                        "label": "Model v3.0 (DL)",
                        "data": [93, 94, 95, 96],
                        "color": "#10b981",  # Green
                    },
                ],
            },
            "false_positive_rates": {
                "labels": weeks,
                "datasets": [
                    {
                        "label": "Model v1.0",
                        "data": [14, 13, 12, 12],
                        "color": "#ef4444",
                    },
                    {
                        "label": "Model v2.1",
                        "data": [8, 7, 6.5, 6],
                        "color": "#3b82f6",
                    },
                    {
                        "label": "Model v3.0",
                        "data": [5, 4, 3.5, 3],
                        "color": "#10b981",
                    },
                ],
            },
        }
    
    def _get_best_model(self) -> Dict:
        """Determine the best performing model"""
        return {
            "id": "dl_model",
            "name": "Deep Learning Model",
            "version": "3.0.0",
            "reason": "96% accuracy with only 3% false positive rate",
            "recommendation": "Consider the trade-off between accuracy and processing time for real-time applications."
        }
    
    def reset_all_metrics(self):
        """Reset performance tracking for all models"""
        for model in self.models.values():
            if hasattr(model, 'reset_history'):
                model.reset_history()
            if hasattr(model, 'total_predictions'):
                model.total_predictions = 0
                model.fraud_detected = 0
